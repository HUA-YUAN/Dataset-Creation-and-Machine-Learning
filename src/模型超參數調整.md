<h2 align="center">
Step5: 交叉驗證方法
</h2>


### 1. 模型超參數調整


模型的超參數設定會直接影響模型結構和效能，而這些超參數都會於訓練模型之前透過手動的方式設定，
若想要更快速且自動的找到最佳的設定值，可以使用一些演算方法來進行。


下方範例將會使用網格搜索(Grid Search)的方式，
並選擇隨機森林超參數中的 n_estimators 與 max_depth 作為調整目標，
對兩者設定一適當的範圍，並根據 [Step5](交叉驗證方法.md)交叉驗證中各個驗證集的平均誤差，
透過迭代來找出最佳超參數組合。


```python
def regressor_gridsearch(x_train_val, y_train_val, stratified, for_stratified_magnitude, mode, feature, output_mode, smote_mode):
    best_score = 0.0
    
    gridsearch_val_mae = []
    gridsearch_max_depth = []
    gridsearch_n_estimators = []
    gridsearch_seed = []
    
    for seed in range(7,8,1):
        
        for max_depth in range(6,7,1):
            
            gridsearch_draw_n_estimators = []
            gridsearch_draw_train_mae = []
            gridsearch_draw_val_mae = []      
            
            #for n_estimators in [25, 50, 75, 100, 150, 250, 300]:
            for n_estimators in range(112,113,1):
 
                train_score, val_score = regressor_cross_validation(seed, max_depth, n_estimators, feature, y_train_val, stratified, for_stratified_magnitude)
           
                gridsearch_val_mae.append(val_score)
                gridsearch_max_depth.append(max_depth)
                gridsearch_n_estimators.append(n_estimators)
                gridsearch_seed.append(seed)
                
                gridsearch_draw_train_mae.append(train_score)
                gridsearch_draw_val_mae.append(val_score)
                gridsearch_draw_n_estimators.append(n_estimators)
      
                if best_score == 0.0:
                    best_score = val_score
                    best_parameters = {"seed":seed,"max_depth":max_depth,"n_estimators":n_estimators,"mae": val_score}
                if val_score < best_score:
                    best_score = val_score
                    best_parameters = {"seed":seed,"max_depth":max_depth,"n_estimators":n_estimators,"mae": val_score}
            
            draw_regressor_gridsearch_mae(seed, max_depth, gridsearch_draw_n_estimators, gridsearch_draw_train_mae, gridsearch_draw_val_mae)
    
    gridsearch_history = {'seed':seed, 'max_depth':gridsearch_max_depth,'n_estimators': gridsearch_n_estimators,'mae_score': gridsearch_val_mae,}
    
    return best_parameters, gridsearch_history
```

最後，我們可以透過上方迭代的歷史紀錄，繪製一個迭代過程的預測結果的變化圖。

```python
def draw_regressor_gridsearch_mae(seed, max_depth, gridsearch_draw_n_estimators, gridsearch_draw_train_mae, gridsearch_draw_val_mae):
    fig, ax = plt.subplots(dpi=150)
    plt.plot(gridsearch_draw_n_estimators, gridsearch_draw_train_mae, 'b', label = 'Training Mean Mae', color='#6495ED')
    plt.plot(gridsearch_draw_n_estimators, gridsearch_draw_val_mae, 'b', label = 'Validation Mean Mae', color='coral')
    plt.title('Gridsearch History (Mean Mae)')
    plt.xlabel('n_estimators')
    plt.ylabel('Mean Mae')
    plt.legend(title='seed = '+ str(seed) + ' ,max_depth = '+ str(max_depth))
    #plt.grid()
    plt.xticks()
    plt.show()
```


![image](/images/StratifiedKfold之流程示意圖.png) 
